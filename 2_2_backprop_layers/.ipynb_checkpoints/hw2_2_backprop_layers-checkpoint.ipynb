{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMVkqpoEOoD3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Numpy-имплементация многослойной нейронной сети\n",
    "\n",
    "**Разработчик: Артём Бабенко**\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mryab/dl-hse-ami/blob/master/week01_intro/homework.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmTxOp7KOoEG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В этом задании необходимо будет самостоятельно реализовать нейронную сеть и обучить ее с помощью метода обратного распространения ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hq9Rs2FYOoEI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lmfLBp4tOoEN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "util.py               0%[                    ]       0  --.-KB/s               \r",
      "util.py             100%[===================>]   3,97K  --.-KB/s    in 0s      \r\n"
     ]
    }
   ],
   "source": [
    "# Load dependencies\n",
    "!wget --quiet --show-progress \"https://raw.githubusercontent.com/mryab/dl-hse-ami/master/week01_intro/util.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqWkDNy2OoEP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Сначала определим базовый класс Layer с основными методами .forward() и .backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jYZ3ptaiOoER",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "\n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "\n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "\n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "\n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "\n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "\n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "\n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = input.shape[1]\n",
    "\n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "\n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYxZM3_jOoEX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Дальнейший план\n",
    "\n",
    "В этом задании необходимо построить нейросеть для классификации рукописных цифр. Для этого понадобится реализовать следующее:\n",
    "- ReLU слой\n",
    "- Полносвязный слой, $f(X)=W \\cdot X + b$\n",
    "- Функцию потерь, для задачи классификации - cross-entropy\n",
    "- Алгоритм обратного распространения ошибки\n",
    "\n",
    "Далее вам необходимо будет реализовать каждый из пунктов выше, пользуяюсь предоставленными сниппетами кода.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_H-OBg_OoEc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ReLU слой\n",
    "В этом слое применяется поэлементная нелинейность $f(X)=max(X,0)$. В этом слое необходимо реализовать только .forward() метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Qe246l61OoEe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
    "        output = np.maximum(input,0)\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        relu_grad_mask = input > 0\n",
    "        return grad_output * relu_grad_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrVM0QqBOoEh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Для отладки градиентов, вычисленных аналитически, рекомендуется использовать сравнение с градиентами, вычисленными численно, как рассказывалось на лекции. Если вы корректно реализовали методы .forward() и .backward(), то значения градиентов окажутся достаточно близкими. Функция численного подсчета градиентов уже реализована в файле util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GeCOrhV0OoEi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# some tests\n",
    "from util import eval_numerical_gradient\n",
    "points = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = ReLU()\n",
    "grads = l.backward(points, np.ones([10,32])/(32*10))\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=points)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
    "    \"gradient returned by your layer does not match the numerically computed gradient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f90WSTdOoEk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Полносвязный слой\n",
    "\n",
    "У полносвязного слоя есть обучаемые параметры: матрица линейного преобразования и столбец свободных членов.\n",
    "\n",
    "$$f(X)= W \\cdot X + b $$\n",
    "\n",
    "* X - матрица входных данных размера [batch_size, num_features],\n",
    "* W - матрица преобразования размера [num_features, num_outputs]\n",
    "* b - столбец свободных членов размера [num_outputs]\n",
    "\n",
    "W и b необходимо инициализировать во время создания слоя и обновлять при каждом вызове метода .backward().\n",
    "Для этого слоя вам необходимо реализовать как прямой, так и обратный проход. Формулы для аналитического подсчета градиентов можно взять из лекции, но рекомендуется проверить их самостоятельно на бумаге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "g22Gzs_2OoEl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "\n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "\n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        return input @ self.weights + self.biases\n",
    "\n",
    "    def backward(self,input,grad_output):\n",
    "\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = grad_output @ self.weights.T\n",
    "\n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = input.T @ grad_output\n",
    "        grad_biases = grad_output.sum(axis=0)\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        # Here we perform a stochastic gradient descent step.\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7zf2tSNOoEn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Тестирование полносвязного слоя\n",
    "\n",
    "Три следующих блока тестируют вашу реализацию, если все корректно, вы трижды получите \"Well done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3XrSvHlvOoEp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "l = Dense(128, 150)\n",
    "\n",
    "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
    "    \"The initial weights must have zero mean and small variance. \"\\\n",
    "    \"If you know what you're doing, remove this assertion.\"\n",
    "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
    "\n",
    "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
    "l = Dense(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.biases = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lnEjmImaOoEx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# To test the grads, we use gradients obtained via finite differences\n",
    "\n",
    "from util import eval_numerical_gradient\n",
    "\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = Dense(32,64,learning_rate=0)\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
    "grads = l.backward(x,np.ones([10,64]))\n",
    "\n",
    "assert np.allclose(grads,numeric_grads,rtol=1e-3,atol=0), \"input gradient does not match numeric grad\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OxArZthGOoEz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "#test gradients w.r.t. params\n",
    "def compute_out_given_wb(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    return l.forward(x)\n",
    "\n",
    "def compute_grad_by_params(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    l.backward(x,np.ones([10,64]) / 10.)\n",
    "    return w - l.weights, b - l.biases\n",
    "\n",
    "w,b = np.random.randn(32,64), np.linspace(-1,1,64)\n",
    "\n",
    "numeric_dw = eval_numerical_gradient(lambda w: compute_out_given_wb(w,b).mean(0).sum(),w )\n",
    "numeric_db = eval_numerical_gradient(lambda b: compute_out_given_wb(w,b).mean(0).sum(),b )\n",
    "grad_w,grad_b = compute_grad_by_params(w,b)\n",
    "\n",
    "assert np.allclose(numeric_dw,grad_w,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
    "assert np.allclose(numeric_db,grad_b,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swZQfJavOoE0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Функция потерь\n",
    "\n",
    "Вычисление функции потерь для задачи классификации, а также ее градиента уже сделано для вас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iCuNsUikOoE1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "\n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "\n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "\n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "n5XRS2i3OoE3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logits = np.linspace(-1,1,500).reshape([50,10])\n",
    "answers = np.arange(50)%10\n",
    "\n",
    "softmax_crossentropy_with_logits(logits,answers)\n",
    "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
    "numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits)\n",
    "\n",
    "assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0), \"The reference implementation has just failed. Someone has just changed the rules of math.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlEURTE9OoE5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Итоговая нейросеть\n",
    "\n",
    "Все готово для запуска нейросети. Нейросеть будем тестировать на классическом датасете MNIST. Код ниже визуализирует несколько примеров из этого датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2KKQ81e0OoE6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/datasets/_openml.py:935: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# doesnt work as yann.lecun site returns 403 and have cert expired\n",
    "# from util import load_mnist\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=True)\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "print (X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 9]\n"
     ]
    }
   ],
   "source": [
    "y = y.astype(np.int64)\n",
    "print(y[2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = X[:50000], X[50000:60000], X[60000:]\n",
    "y_train, y_val, y_test = y[:50000], y[50000:60000], y[60000:]\n",
    "\n",
    "X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "X_test = X_test.reshape([X_test.shape[0], -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %s\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGXtnvX4OoE7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В нашей реализации сеть - просто список (Python-list) слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VicezF_TOoE8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 40\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "482hIf_UOoE9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Реализуйте прямой проход по целой сети, последовательно вызывая .forward() для каждого слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKRyUyj5OoE9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    \"\"\"\n",
    "    Compute activations of all network layers by applying them sequentially.\n",
    "    Return a list of activations for each layer.\n",
    "    Make sure last activation corresponds to network logits.\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    input = X\n",
    "\n",
    "    for layer in network:\n",
    "        activations.append(layer.forward(input))\n",
    "        input = activations[-1]\n",
    "\n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    Use network to predict the most likely class for each sample.\n",
    "    \"\"\"\n",
    "    logits = forward(network, X)[-1]\n",
    "    return logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfUyHKPyOoE-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(network,X,y):\n",
    "    \"\"\"\n",
    "    Train your network on a given batch of X and y.\n",
    "    You first need to run forward to get all layer activations.\n",
    "    Then you can run layer.backward going from last to first layer.\n",
    "\n",
    "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network,X)\n",
    "    layer_inputs = [X] + layer_activations  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "\n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "\n",
    "    # propagate gradients through network layers using .backward\n",
    "    # hint: start from last layer and move to earlier layers\n",
    "    for i in reversed(range(len(network))):\n",
    "        loss_grad = network[i].backward(layer_inputs[i], loss_grad)\n",
    "    \n",
    "\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJG4VMsROoE_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Все готово для запуска обучения. Если все реализовано корректно, то точность классификации на валидационном множестве должна превысить 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kq5XTDNNOoE_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in tqdm(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q2KcwkTOoFA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaxQu9WsOoFB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(15):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
    "        train(network, x_batch, y_batch)\n",
    "\n",
    "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
    "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
    "\n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864, 0.09864]\n"
     ]
    }
   ],
   "source": [
    "print(train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9fC5uXsOoFC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Нейросеть для извлечения представлений\n",
    "Помимо собственно классификации, нейросети часто используют для получения векторных описаний (embeddings) объектов различной природы. Часто в качестве представлений можно взять просто векторы активаций нейронов с одного из последних слоев. Вычислим такие представления:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG6YMj4ZOoFC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_activations = forward(network, X_train)\n",
    "train_hidden_representations = train_activations[2]\n",
    "test_activations = forward(network, X_test)\n",
    "test_hidden_representations = test_activations[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUHfj-dPOoFD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Полученные представления можно использовать в любых целях, в комбинации с другими инструментами машинного обучения.\n",
    "Сравним обученные представления для изображений из MNIST c исходными 784-мерными представлениями. Постройте t-SNE визуализации 1000 случайных изображений из X_train, полученные на основе исходных 784-мерных представлений и обученных представлений. Сделайте вывод о качестве представлений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnnHe2ciOoFE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from util import plot_embedding\n",
    "from sklearn.manifold import TSNE\n",
    "subset_ids = np.random.permutation(X_train.shape[0])[:1000]\n",
    "#x_original_tsne = <your code>\n",
    "#x_learnt_tsne = <your code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu656iCXOoFE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_embedding(x_original_tsne, y_train[subset_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfYV0ysmOoFF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_embedding(x_learnt_tsne, y_train[subset_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj1MvHJZOoFG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Наконец, исследуем универсальность полученных представлений. Используем их для решения другой задачи классификации: предсказание четности изображенной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7ZnVRq4OoFH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_y_train = y_train % 2\n",
    "new_y_test = y_test % 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_54iXFQYOoFI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Обучите простейшую логистическую регресиию, предсказывающую четность изображенной цифры, используя в качестве вектора признаков:\n",
    "\n",
    "1) исходные 784-мерные представления\n",
    "\n",
    "2) обученные представления\n",
    "\n",
    "Сравните качество и сделайте вывод об универсальности обученных представлений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgxNwiBcOoFJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver=\"lbfgs\")\n",
    "#<your code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHU7TfIdOoFK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/mryab/dl-hse-ami/blob/master/week01_intro/homework.ipynb",
     "timestamp": 1670041484038
    }
   ]
  },
  "hse_dl_year": "2021-fall",
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
