{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ3DJzu7FDX_"
   },
   "source": [
    "# Домашнее задание 2.1\n",
    "\n",
    "В этом задании вы должны:\n",
    "1. Написать слой Conv2d на Numpy и определить в нем forward-backward методы\n",
    "2. Определить слой MaxPool2d\n",
    "3. Написать всю необходимую обвязку для обучения: оптимизатор с адаптивным шагом и класс, позволяющий изменять расписание для learning rate'а\n",
    "\n",
    "> Обратите внимание, что в этом задании больше нет тестов.\n",
    "> Вы должны сами проверять свой код.  \n",
    "> Это можно сделать так:\n",
    "> 1. Написать юнит-тесты с помощью Pytorch. То есть, ваш модудь должен повторять поведение torch'а\n",
    "> 2. Проверять архитектуру не на всем датасете (одна эпоха в наивной имплементации будет занимать около двух часов), а на подвыборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMVkqpoEOoD3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Numpy-имплементация сверточной нейронной сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmTxOp7KOoEG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Вставьте сюда имплементацию из первого домашнего задания.\n",
    "\n",
    "\n",
    "\n",
    "> Обратите внимание, что обновление весов теперь производится с помощью специального класса **Optimizer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hq9Rs2FYOoEI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lmfLBp4tOoEN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def load_mnist_legacy(flatten=False):\n",
    "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "def load_mnist(flatten):\n",
    "    X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "    X_train, X_val, X_test = X[:50000], X[50000:60000], X[60000:]\n",
    "    y_train, y_val, y_test = y[:50000], y[50000:60000], y[60000:]\n",
    "\n",
    "    X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "    X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "    X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jYZ3ptaiOoER",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "\n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "\n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "\n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "\n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "\n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "\n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "\n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "        input_dim = input.shape[1]\n",
    "\n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "\n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Qe246l61OoEe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
    "        output = np.maximum(input,0)\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        relu_grad_mask = input > 0\n",
    "        return grad_output * relu_grad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "g22Gzs_2OoEl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "\n",
    "        self.grad_weights = None\n",
    "        self.grad_biases = None\n",
    "\n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "\n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        return input @ self.weights + self.biases\n",
    "\n",
    "    def backward(self,input,grad_output):\n",
    "        grad_input = grad_output @ self.weights.T\n",
    "\n",
    "        grad_weights = input.T @ grad_output\n",
    "        grad_biases = grad_output.sum(axis=0)\n",
    "\n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "\n",
    "        self.grad_weights = grad_weights\n",
    "        self.grad_biases = grad_biases\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1vN8h41ILY9"
   },
   "outputs": [],
   "source": [
    "class Conv2d(Layer):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size):\n",
    "\n",
    "        self.weights = np.random.randn(input_channels, output_channels, kernel_size, kernel_size)*0.01\n",
    "        self.biases = np.zeros(output_channels)\n",
    "\n",
    "        self.grad_weights = None\n",
    "        self.grad_biases = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an convolution:\n",
    "\n",
    "        output_height = input_height - kernel_size + 1\n",
    "        output_width = input_width - kernel_size + 1\n",
    "\n",
    "        input shape: [batch, input_channels, input_height, input_width]\n",
    "        output shape: [batch, output_channels, output_height, output_width]\n",
    "        \"\"\"\n",
    "        batch_size, input_channels, input_height, input_width = input.shape\n",
    "        output_channels, k_h, k_w = self.weight.shape[1:]\n",
    "\n",
    "        output_height = input_height - self.weight.shape[2] + 1\n",
    "        output_width = input_width - self.weight.shape[3] + 1\n",
    "        output = np.empty((batch_size, output_channels, output_height, output_width))\n",
    "\n",
    "        for out_h in range(output_height):\n",
    "            for out_w in range(output_width):\n",
    "                    step_h, step_w = out_h+k_h, out_w+k_w\n",
    "                    input_region = input[..., None, out_h:step_h, out_w:step_w]\n",
    "                    output[..., out_h, out_w] = np.sum(input_region * self.weight, axis=(1,3,4))\n",
    "\n",
    "        output += self.bias[None, :, None, None]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        dL/df = conv(input, grad_output)\n",
    "        \n",
    "        dL/dX = F_i^T @ grad_output\n",
    "        \"\"\"\n",
    "        \n",
    "        #grad_input = <your code here>\n",
    "\n",
    "        #grad_weights = <your code here>\n",
    "        #grad_biases = <your code here>\n",
    "\n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "\n",
    "        self.grad_weights = grad_weights\n",
    "        self.grad_biases = grad_biases\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1iE85PpMm0Z"
   },
   "outputs": [],
   "source": [
    "class MaxPool2d(Layer):\n",
    "    def __init__(self, kernel_size):\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an pooling:\n",
    "\n",
    "        output_height = input_height // kernel_size\n",
    "        output_width = input_width // kernel_size\n",
    "\n",
    "        input shape: [batch, input_channels, input_height, input_width]\n",
    "        output shape: [batch, input_channels, output_height, output_width]\n",
    "        \"\"\"\n",
    "        batch_size, input_channels, input_height, input_width = input.shape\n",
    "        \n",
    "        # lazy impl implying input_channels is multiple of kernel\n",
    "        output_height = input_height // kernel_size\n",
    "        output_width = input_width // kernel_size\n",
    "        output_channels = input_channels\n",
    "        output = np.empty((batch_size, output_channels, output_height, output_width))\n",
    "        \n",
    "        # 0 for h, 1 for w of max\n",
    "        self.indices =  np.empty(output.shape))\n",
    "        \n",
    "        # i'm too stupid to vectorize argmax..\n",
    "        for n in range(batch_size):\n",
    "            for c in range(input_channels):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "\n",
    "                        shift_h = h * self.kernel.size\n",
    "                        shift_w = w * self.kernel.size\n",
    "\n",
    "                        input_region = input[n, c, shift_h:self.kernel_size, shift_w:self.kernel_size]\n",
    "                        output[n, c, h, w] = np.max(input_region)\n",
    "\n",
    "                        scalar_ind = np.argmax(input_region)\n",
    "                        ind = np.unravel_index(scalar_ind, (self.kernel_size, self.kernel_size))\n",
    "\n",
    "                        self.indices[n,c,h,w] = (ind[0]+shift_h, ind[1]+shift_w)\n",
    "                    \n",
    "        return output\n",
    "                    \n",
    "    def backward(self, input, grad_output):\n",
    "        grad_input = np.zeros_like(input)\n",
    "        batch_size, input_channels, output_height, output_width = grad_output.shape\n",
    "        \n",
    "        for n in range(batch_size):\n",
    "            for c in range(input_channels):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "                        \n",
    "                    ind = self.indices[n,c,h,w]\n",
    "                    grad_input[n,c,ind[0],ind[1]] = grad_output[n,c,h,w] \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3W-GM45xSqJD"
   },
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    def forward(self, input):\n",
    "          \"\"\"\n",
    "          Perform an flatten operation:\n",
    "\n",
    "          input shape: [batch, input_channels, input_height, input_width]\n",
    "          output shape: [batch, input_channels * output_height * output_width]\n",
    "          \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \n",
    "        grad_input = grad_output.reshape(input.shape)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCuNsUikOoE1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "\n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "\n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "\n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muokQIA3UcnL"
   },
   "source": [
    "## Имплементация оптимизатора и изменения learning rate'a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBzMVan_XzFM"
   },
   "source": [
    "В имплементации этих двух классов есть небольшие неточности.\n",
    "Посмотрите, как сделана имплементация метода моментов в Pytorch и добавьте пропущенное.\n",
    "\n",
    "Если хотите, можете имплементировать какой-нибудь другой оптимизатор - например, AdamW, Sophia, Lion, Adan, LAMB и т.п.\n",
    "\n",
    "Также можете придумать нестандартный scheduling (подсказка: нестандартные расписания лучше всего искать во вполне стандартных научных статьях).\n",
    "\n",
    "За это будут дополнительные баллы. Но обязательно нужно сравнить имплементированный вами оптимизатор со стандартным SGD и методом мом\n",
    "\n",
    "> Добавлять моменты Нестерова не нужно!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4hyzuIcUqLd"
   },
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, momentum=0.9, dampening=0.0, weight_decay=0.0, lr=0.1, params=1):\n",
    "        \"\"\"\n",
    "        Wrapper which perfoms weights update\n",
    "        \"\"\"\n",
    "        self.momentum = momentum\n",
    "        self.dampening = dampening\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr = lr\n",
    "\n",
    "        # здесь будем копить моментумы\n",
    "        # и будем делать это для каждого слоя\n",
    "        self.momentum_buffer = np.zeros(params)\n",
    "\n",
    "    def step(self, layers):\n",
    "        \"\"\"\n",
    "        Update weights\n",
    "        in-place for each layer\n",
    "        \"\"\"\n",
    "        for i in range(len(layers)):\n",
    "            grad_weights = layers[i].grad_weights\n",
    "            self.momentum_buffer[i] = self.momentum * self.momentum_buffer[i] + (1 - self.dampening) * grad_weights\n",
    "            layers[i].weights = layers[i].weights - lr * self.momentum_buffer[i]\n",
    "\n",
    "        #<your code here>\n",
    "\n",
    "        #return weights - lr * self.momentum_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DX-leurYblp"
   },
   "outputs": [],
   "source": [
    "class ExponentialLRScheduler:\n",
    "    def __init__(self, lr, optimizer, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Wrapper which perfoms learning rate updates\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.current_step = 0\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update learing rate for current iteration\n",
    "        \"\"\"\n",
    "        current_lr = self.lr * self.gamma\n",
    "        \n",
    "        optimizer.lr = current_lr\n",
    "        \n",
    "        #self.current_step += 1\n",
    "        #return current_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlEURTE9OoE5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Итоговая нейросеть\n",
    "\n",
    "Все готово для запуска нейросети. Нейросеть будем тестировать на классическом датасете MNIST. Код ниже визуализирует несколько примеров из этого датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KKQ81e0OoE6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=False)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGXtnvX4OoE7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В нашей реализации сеть - просто список (Python-list) слоев.\n",
    "\n",
    "\n",
    "\n",
    "> Обратите внимание, что у нас нет глобального пулинга. При изменении архитектуры сети вы должны поменять входую размерность в последнем Dense слое\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VicezF_TOoE8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 10\n",
    "network.append(Conv2d(1, 8, 5))\n",
    "network.append(MaxPool2d(2))\n",
    "network.append(ReLU())\n",
    "network.append(Conv2d(8, 16, 3))\n",
    "network.append(MaxPool2d(2))\n",
    "network.append(ReLU())\n",
    "network.append(Flatten())\n",
    "network.append(Dense(6 * 16, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sUrd667ZDuK"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = SGDOptimizer(params=3)\n",
    "scheduler = ExponentialLRScheduler(learning_rate, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "482hIf_UOoE9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Реализуйте прямой проход по целой сети, последовательно вызывая .forward() для каждого слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKRyUyj5OoE9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    \"\"\"\n",
    "    Compute activations of all network layers by applying them sequentially.\n",
    "    Return a list of activations for each layer.\n",
    "    Make sure last activation corresponds to network logits.\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    input = X\n",
    "\n",
    "    for layer in network:\n",
    "        activations.append(layer.forward(input))\n",
    "        input = activations[-1]\n",
    "\n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    Use network to predict the most likely class for each sample.\n",
    "    \"\"\"\n",
    "    logits = forward(network, X)[-1]\n",
    "    return logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfUyHKPyOoE-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(network,X,y):\n",
    "    \"\"\"\n",
    "    Train your network on a given batch of X and y.\n",
    "    You first need to run forward to get all layer activations.\n",
    "    Then you can run layer.backward going from last to first layer.\n",
    "\n",
    "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network,X)\n",
    "    layer_inputs = [X] + layer_activations  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "\n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "\n",
    "    # propagate gradients through network layers using .backward\n",
    "    # hint: start from last layer and move to earlier layers\n",
    "    for i in reversed(range(len(network))):\n",
    "        loss_grad = network[i].backward(layer_inputs[i], loss_grad)\n",
    "\n",
    "    # update weights and biases with optimizer\n",
    "    optimizer.step([network[0], network[3], network[7]])\n",
    "\n",
    "    # update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJG4VMsROoE_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Все готово для запуска обучения. Если все реализовано корректно, то точность классификации на валидационном множестве **должна превысить** 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kq5XTDNNOoE_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in tqdm(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q2KcwkTOoFA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaxQu9WsOoFB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(15):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
    "        train(network, x_batch, y_batch)\n",
    "\n",
    "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
    "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
    "\n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMMVICGXB4Xdl4EmZfdsmPJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
